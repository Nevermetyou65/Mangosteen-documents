{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5662b4a0-fea5-4168-82f0-c77a0c59facb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa244620-8f57-43e5-b053-df1b430f274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import torch\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import joblib\n",
    "\n",
    "from langdetect import DetectorFactory, detect_langs\n",
    "from datetime import datetime\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from compute_ppl import compute_ppl\n",
    "from transformers import AutoModelForCausalLM, BarthezTokenizer\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba25876-3671-459d-afb0-2b3c7241327a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_md(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        md_content = f.read()\n",
    "    return md_content\n",
    "\n",
    "\n",
    "def open_json(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        json_meta = json.load(f)\n",
    "    return json_meta\n",
    "\n",
    "\n",
    "def get_current_datetime():\n",
    "    \"\"\"Get current date and time\"\"\"\n",
    "    now = datetime.now()\n",
    "\n",
    "    # Format the date-time string\n",
    "    formatted_datetime = now.strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "\n",
    "    return formatted_datetime\n",
    "\n",
    "\n",
    "def setup_logger(\n",
    "    prefix: str,\n",
    "    console_level: str = \"DEBUG\",\n",
    "    file_level: str = \"WARNING\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sets up a logger with a console and file handler.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(\n",
    "        logging.DEBUG\n",
    "    )  # Set to the highest level; handlers will filter appropriately\n",
    "\n",
    "    # Create console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(console_level)\n",
    "\n",
    "    log_file = f\"../log/{prefix}_{get_current_datetime()}.log\"\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file, mode=\"a\", encoding=\"utf-8\")\n",
    "    file_handler.setLevel(file_level)\n",
    "\n",
    "    # Define log format\n",
    "    formatter = logging.Formatter(\n",
    "        \"{asctime},{levelname},{message}\",\n",
    "        style=\"{\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    # Assign formatter to handlers\n",
    "    console_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbab812-40a5-48a0-9e85-efcb3974bc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_picture(json_files, logger):\n",
    "    \"\"\"Filter PDFs where the number of 'Picture' blocks per page is less than 1.\"\"\"\n",
    "    remain = []\n",
    "\n",
    "    for file_path in tqdm(json_files):\n",
    "        try:\n",
    "            data = open_json(file_path)\n",
    "            page_stats = data.get(\"page_stats\", [])\n",
    "\n",
    "            if not page_stats:\n",
    "                logger.warning(f\"No page stats found in {pdf_name}, skipping.\")\n",
    "                continue\n",
    "\n",
    "            num_all_pages = len(page_stats)\n",
    "\n",
    "            # Collect all block counts across pages\n",
    "            block_counts = [\n",
    "                block for page in page_stats for block in page.get(\"block_counts\", [])\n",
    "            ]\n",
    "\n",
    "            # Count the total number of 'Picture' blocks\n",
    "            num_all_pic = sum(\n",
    "                count for block, count in block_counts if block == \"Picture\"\n",
    "            )\n",
    "\n",
    "            # Compute the ratio and filter PDFs\n",
    "            if num_all_pages > 0 and (num_all_pic / num_all_pages) < 1:\n",
    "                remain.append(file_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing {pdf_name}: {e}\")\n",
    "\n",
    "    logger.info(f\"Num remain: {len(remain)}\")\n",
    "    logger.info(f\"Num filter: {len(json_files) - len(remain)}\")\n",
    "\n",
    "    return remain\n",
    "\n",
    "\n",
    "def filter_by_lang(md_files, logger):\n",
    "    \"\"\"Filter PDFs that are detected to be in Thai with a probability > 0.5.\"\"\"\n",
    "    remain = []\n",
    "\n",
    "    for md_path in tqdm(md_files):\n",
    "        try:\n",
    "            md_content = open_md(md_path)\n",
    "\n",
    "            # Detect language probabilities\n",
    "            lang_probs = [\n",
    "                lang.prob for lang in detect_langs(md_content) if lang.lang == \"th\"\n",
    "            ]\n",
    "\n",
    "            # If Thai language probability is greater than 0.5, keep the document\n",
    "            if lang_probs and lang_probs[0] > 0.5:\n",
    "                remain.append(md_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cannot read OCR result for {md_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "    logger.info(f\"Num remain: {len(remain)}\")\n",
    "    logger.info(f\"Num filter: {len(md_files) - len(remain)}\")\n",
    "\n",
    "    return remain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d991d5-5ce8-4c68-a701-15db88a0bc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b675041a-8121-4aed-a0e2-bd35a9987599",
   "metadata": {},
   "source": [
    "# Filter by image and language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fe1d1d-db1a-4abc-bfed-ecfc7d5e6887",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_files = glob.glob(\"../ocr_result/*/*.md\")\n",
    "json_files = glob.glob(\"../ocr_result/*/*.json\")\n",
    "print(f\"Num pdf: {len(md_files)}\")\n",
    "assert len(md_files) == len(json_files)\n",
    "\n",
    "logger = setup_logger(prefix=\"ocr_filter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63e2f8-bfbf-4791-9628-e455a39733e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "remain = filter_by_picture(json_files=json_files, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce71a24-d48c-494c-a8a1-0738be3952bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remain = [p.replace(\"_meta.json\", \".md\") for p in remain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17756d72-e2da-48c7-8131-d723084e64b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "remain = filter_by_lang(md_files=remain, logger=logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f82a4d2-c534-496a-94ba-13cf3ff9f47a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.Series(remain).to_csv(\"./filtered_md.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65e69e-4eda-42c6-aa29-17c5702f15bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2212877a-a9a6-41ac-8511-ccc56340ec38",
   "metadata": {},
   "source": [
    "# Filter by perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257e20c-11ec-4763-80d5-193ebb2713a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "remain = pd.read_csv(\"./filtered_md.csv\").iloc[:, 0].to_list()\n",
    "logger = setup_logger(prefix=\"ocr_filter_ppl\")\n",
    "\n",
    "model_name = \"airesearch/wangchanbart-base\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "assert device == \"cuda\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = BarthezTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615679be-766a-405e-959a-489fd9c12624",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_perplexity(\n",
    "    max_length, stride, list_of_path, model, tokenizer, logger, device\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes the perplexity for a list of text files and returns a list of tuples (file_path, perplexity).\n",
    "    \"\"\"\n",
    "    ppl_list = []\n",
    "\n",
    "    print(f\"Number of GPUs used: {torch.cuda.device_count()}\")\n",
    "\n",
    "    for path in tqdm(list_of_path):\n",
    "        try:\n",
    "            text = open_md(path)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Cannot open {path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            encodings = tokenizer(text, return_tensors=\"pt\")\n",
    "            seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "            _, ppl = compute_ppl(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                encodings,\n",
    "                max_length=max_length,\n",
    "                seq_len=seq_len,\n",
    "                stride=stride,\n",
    "                device=device,\n",
    "            )\n",
    "\n",
    "            ppl_list.append((path, ppl))\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error computing perplexity for {path}: {e}\")\n",
    "\n",
    "    return ppl_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc0eabe-4e53-4421-8365-a6f52050cd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = model.config.max_position_embeddings\n",
    "stride = 512\n",
    "\n",
    "ppl_list = compute_perplexity(\n",
    "    max_length=max_length,\n",
    "    stride=stride,\n",
    "    list_of_path=remain,\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    device=device,\n",
    "    logger=logger,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b5d3f-b987-4cf4-82e1-b3250ad02c52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppl_df = pd.DataFrame(ppl_list, columns=[\"file_path\", \"ppl_score\"])\n",
    "ppl_df.to_csv(\"ppl_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308383ed-696b-48bc-8b75-775974c4219a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl_df = pd.read_csv(\"./ppl_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3979f1e-7962-474e-8a83-ec7d8da8f993",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    ppl_df[\"ppl_score\"]\n",
    "    .describe(percentiles=[0.1, 0.3, 0.5, 0.7, 0.9, 0.99, 0.999])\n",
    "    .to_frame()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f0ee3-a7ef-4558-802b-dd08d8ab698e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with two subplots side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# First histogram\n",
    "axes[0].hist(ppl_df[\"ppl_score\"], bins=\"auto\")\n",
    "axes[0].set_title(\"PPL score\")\n",
    "\n",
    "# Second histogram with x-axis limit\n",
    "axes[1].hist(ppl_df[\"ppl_score\"], bins=\"auto\")\n",
    "axes[1].set_xlim(0, 3200)\n",
    "axes[1].set_title(\"PPL score (Limited x-axis)\")\n",
    "\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea770403-572e-46d3-ae84-c29ffcdeb855",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = ppl_df.loc[ppl_df[\"ppl_score\"].lt(1500)].copy()\n",
    "filtered[\"file_path\"] = filtered[\"file_path\"].str.replace(r\"../\", \"\", regex=False)\n",
    "filtered.to_csv(\"../ppl_filtered_md.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33efca0a-07ad-4c73-9606-ff967ed0f614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fc8b9f7-be49-44af-a89e-01d615fede9e",
   "metadata": {},
   "source": [
    "# LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506788ac-4fef-415b-8420-cded5666ed31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "md = pd.read_csv(\"../ppl_filtered_md.csv\")\n",
    "\n",
    "openai_api_key = \"EMPTY\"\n",
    "llama_api = \"http://10.204.100.76:11000/v1\"\n",
    "qwen_api = \"http://10.204.100.79:11700/v1\"\n",
    "\n",
    "llama_client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=llama_api,\n",
    ")\n",
    "\n",
    "qwen_client = OpenAI(\n",
    "    api_key=openai_api_key,\n",
    "    base_url=qwen_api,\n",
    ")\n",
    "\n",
    "llama_model = \"hugging-quants/Meta-Llama-3.1-405B-Instruct-AWQ-INT4\"\n",
    "qwen_model = \"Qwen/Qwen2.5-72B-Instruct\"\n",
    "\n",
    "\n",
    "def get_output(client, model: str, messages: list[dict[str, str]]):\n",
    "    chat_response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return chat_response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21408505-63bf-4637-a235-3370243ee554",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"\"\"\n",
    "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
    "You specialize in proofreading and refining OCR-extracted Thai text from PDF files while preserving the original context, language, and layout.\n",
    "\n",
    "Your responsibilities:\n",
    "1. **Analyze** the OCR-extracted text.\n",
    "2. **Correct** OCR errors and refine the text for enhanced correctness and readability.\n",
    "3. **Preserve** the original language (Thai, English, or mixed). Output Chinese only if the input is entirely in Chinese.\n",
    "4. **Maintain** the original context, semantic meaning, and overall layout.\n",
    "5. **Preserve** all Markdown formatting including headings, paragraphs, lists, tables, equations, and special symbols.\n",
    "6. **For tables:** Retain the table structure and refine individual cell content as needed.\n",
    "7. **Remove** any redundant, erroneous text, and html tags that does not contribute to the overall meaning or layout.\n",
    "8. **If uncertain** about a correction, leave the original segment unchanged.\n",
    "\n",
    "Return only the final refined text in **Markdown** format without any additional commentary or explanation.\n",
    "\"\"\",\n",
    "}\n",
    "\n",
    "\n",
    "user_content = \"\"\"\n",
    "Your task is to refine and correct the following Thai text extracted via OCR. Ensure you:\n",
    "\n",
    "1. Preserve the original context, language, and layout (including tables and special symbols).\n",
    "2. Maintain all Markdown formatting in your output.\n",
    "3. Remove redundant or erroneous text that does not contribute to the overall meaning.\n",
    "4. Retain any segment as-is if you are uncertain about the correction.\n",
    "5. **Do not** add new headings or sections that do not exist in the input.\n",
    "6. Correct OCR errors strictly without altering the semantic meaning.\n",
    "\n",
    "**Example**\n",
    "\n",
    "INPUT:\n",
    "# ทที่ 3\n",
    "# ่วงโซ่อุปทานและระบบโลจิสติกส์อุตสาหกรรมน้ำมันปาล์ม\n",
    "## .1 ภาพรวมห่วงโซ่อุปทานอุตสาหกรรมน้ำมันปาล์ม\n",
    "าพรวมห่วงโซ่อุปทาน (Supply Chain) อุตสาหกรรมน้ำมันปาล์มพิจารณาตามผลผลิต (Output) พบว่าห่วงโซ่อุปทานอุตสาหกรรมน้ำมันปาล์มของอินโดนีเซีย มาเลเซีย และไทยมีลักษณะ ามลำดับขั้นเป็นไปในลักษณะเดียวกัน กล่าวอีกนัยหนึ่งไม่มีความแตกต่างกัน ห่วงโซ่อูปทาน ียงลำดับเริ่มต้นจากการปลูก (Planting) การสกัด (Milling) การกลั่น (Refinery) และอุตสาหกรรม ลายน้ำ (Downstream) รายละเอียดดังนี้\n",
    "\n",
    "EXPECTED OUTPUT:\n",
    "# บทที่ 3\n",
    "# ห่วงโซ่อุปทานและระบบโลจิสติกส์อุตสาหกรรมน้ำมันปาล์ม\n",
    "## 3.1 ภาพรวมห่วงโซ่อุปทานอุตสาหกรรมน้ำมันปาล์ม\n",
    "ภาพรวมห่วงโซ่อุปทาน (Supply Chain) อุตสาหกรรมน้ำมันปาล์มพิจารณาตามผลผลิต (Output) พบว่าห่วงโซ่อุปทานอุตสาหกรรมน้ำมันปาล์มของอินโดนีเซีย มาเลเซีย และไทยมีลักษณะตามลำดับขั้นเป็นไปในลักษณะเดียวกัน กล่าวอีกนัยหนึ่งไม่มีความแตกต่างกัน ห่วงโซ่อุปทานเรียงลำดับเริ่มต้นจากการปลูก (Planting) การสกัด (Milling) การกลั่น (Refinery) และอุตสาหกรรมปลายน้ำ (Downstream) รายละเอียดดังนี้\n",
    "\n",
    "Below is your input:\n",
    "INPUT:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcafa291-36cc-4641-aa35-4734ed6fcd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5705f59a-3858-4622-908d-88ab356a58f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path = os.path.join(\"..\", \"ocr_result/pdf_doc_00351/pdf_doc_00351.md\")\n",
    "# test = open_md(path)\n",
    "# print(test)\n",
    "\n",
    "# split_pat = re.compile(r\"\\{\\d+\\}\\-{48}\")\n",
    "# test_split = split_pat.split(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac519a-4e6b-4a1d-9ce2-92d9a18f12e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# outout = []\n",
    "# for i, page in enumerate(tqdm(test_split)):\n",
    "#     if page:\n",
    "#         if i > 15:\n",
    "#             break\n",
    "#         user_content_w_input = user_content + page.strip()\n",
    "#         user = {\"role\": \"user\", \"content\": user_content_w_input}\n",
    "#         messages = [system, user]\n",
    "#         llm_response = get_output(\n",
    "#             client=qwen_client,\n",
    "#             model=qwen_model,\n",
    "#             messages=messages,\n",
    "#         )\n",
    "#         outout.append(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43785bb-50f1-45d9-a857-8b0330a05abf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# outout_tmp = [\n",
    "#     f\"{i}------------------------------------------------\\n\" + txt\n",
    "#     for i, txt in enumerate(outout)\n",
    "# ]\n",
    "# print(\"\\n\\n\".join(outout_tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e72d63-06ee-4c0a-aa38-46c12d071f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open a file in write mode (using 'utf-8' encoding to support special characters)\n",
    "# with open(\"../output.md\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     file.write(\"\\n\\n\".join(outout_tmp))\n",
    "\n",
    "# print(\"Markdown file saved as output.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67f0e89-bb99-4153-ada8-6746c2dfafe7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d60632-b88f-4d61-a2c2-b55b9d760db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../pdf_data_2025-02-26_02_25_48_has_sep.jsonl\"\n",
    "jsonl = pd.read_json(path, lines=True, orient=\"records\")\n",
    "logger = setup_logger(\"pdf_llm\", console_level=\"DEBUG\", file_level=\"DEBUG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5064c8-38d6-402a-b600-cc5eefe3079f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# split_pat = re.compile(r\"\\{\\d+\\}\\-{48}\\n\\n\")\n",
    "# result = []\n",
    "# for i, text in enumerate(tqdm(jsonl[\"text\"])):\n",
    "#     logger.info(f\"Processing pdf id {i}\")\n",
    "#     text_split = split_pat.split(text)\n",
    "#     output = []\n",
    "#     for page_no, page in enumerate(text_split):\n",
    "#         if page:\n",
    "#             user_content_w_input = user_content + page.strip()\n",
    "#             user = {\"role\": \"user\", \"content\": user_content_w_input}\n",
    "#             messages = [system, user]\n",
    "#             try:\n",
    "#                 llm_response = get_output(\n",
    "#                     client=qwen_client,\n",
    "#                     model=qwen_model,\n",
    "#                     messages=messages,\n",
    "#                 )\n",
    "#                 output.append(llm_response)\n",
    "#             except Exception as e:\n",
    "#                 logger.error(f\"There is error at pdf id {i}, page no: {page_no}\")\n",
    "#                 logger.error(f\"The error is {e}\")\n",
    "#                 logger.error(\"Will append original page\")\n",
    "#                 output.append(page.strip())\n",
    "\n",
    "\n",
    "#     text_llm = \"\\n\\n\".join(output)\n",
    "#     result.append(text_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7a3f95-7d57-4c8b-8f1b-fe6bd3a6fbe1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a0a72-5f7b-4ada-bc5f-9b069e6502f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(text_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9bb665-e3f2-4dc2-adc5-89d23fd9993e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def process_page(pdf_id: int, page_no: int, page_text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Processes a single page by combining the prompt with the page text,\n",
    "#     sending it to the LLM, and handling any errors.\n",
    "#     \"\"\"\n",
    "#     if not page_text:\n",
    "#         return \"\"\n",
    "#     prompt = user_content + page_text.strip()\n",
    "#     messages = [system, {\"role\": \"user\", \"content\": prompt}]\n",
    "#     try:\n",
    "#         result = get_output(client=qwen_client, model=qwen_model, messages=messages)\n",
    "#     except Exception as e:\n",
    "#         logger.error(f\"Error processing pdf id {pdf_id}, page {page_no}: {e}\")\n",
    "#         result = page_text.strip()  # Fallback: return the original text\n",
    "#     return result\n",
    "\n",
    "\n",
    "# def process_pdf(pdf_id: int, pdf_text: str) -> str:\n",
    "#     \"\"\"\n",
    "#     Splits the PDF text into pages and processes each page concurrently.\n",
    "#     Returns the processed text for the PDF.\n",
    "#     \"\"\"\n",
    "#     logger.info(f\"Processing pdf id {pdf_id}\")\n",
    "#     pages = split_pattern.split(pdf_text)\n",
    "#     processed_pages = [None] * len(pages)\n",
    "\n",
    "#     # Process pages concurrently using multithreading\n",
    "#     with ThreadPoolExecutor(max_workers=8) as page_executor:\n",
    "#         future_to_index = {\n",
    "#             page_executor.submit(process_page, pdf_id, idx, page): idx\n",
    "#             for idx, page in enumerate(pages)\n",
    "#         }\n",
    "#         for future in as_completed(future_to_index):\n",
    "#             idx = future_to_index[future]\n",
    "#             try:\n",
    "#                 processed_pages[idx] = future.result()\n",
    "#             except Exception as e:\n",
    "#                 logger.error(f\"Error in pdf id {pdf_id}, page {idx}: {e}\")\n",
    "#                 processed_pages[idx] = pages[\n",
    "#                     idx\n",
    "#                 ].strip()  # Fallback: original page text\n",
    "#     return \"\\n\\n\".join(processed_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5b1c1d-e212-4fc2-8dba-e99b0b590f95",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# num_pdfs = len(jsonl_df)\n",
    "# results = [None] * num_pdfs\n",
    "\n",
    "# # Process PDFs concurrently using multithreading\n",
    "# with ThreadPoolExecutor(max_workers=32) as pdf_executor:\n",
    "#     future_to_pdf = {\n",
    "#         pdf_executor.submit(process_pdf, idx, text): idx\n",
    "#         for idx, text in enumerate(jsonl_df[\"text\"])\n",
    "#     }\n",
    "#     for future in tqdm(\n",
    "#         as_completed(future_to_pdf), total=len(future_to_pdf), desc=\"Processing PDFs\"\n",
    "#     ):\n",
    "#         idx = future_to_pdf[future]\n",
    "#         try:\n",
    "#             results[idx] = future.result()\n",
    "#         except Exception as e:\n",
    "#             logger.error(f\"Error processing pdf id {idx}: {e}\")\n",
    "#             results[idx] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d1e13-ca27-4480-bbb4-7edfa55eaef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../pdf_data_2025-02-26_02_25_48_has_sep.jsonl\"\n",
    "jsonl_df = pd.read_json(path, lines=True, orient=\"records\")\n",
    "logger = setup_logger(\"pdf_llm\", console_level=\"DEBUG\", file_level=\"DEBUG\")\n",
    "split_pat = re.compile(r\"\\{\\d+\\}\\-{48}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2117ad-7aeb-4da0-be54-cab684a3a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf(pdf_id, pdf_text):\n",
    "    \"\"\"\n",
    "    Process a single PDF:\n",
    "      1. Split its text into pages.\n",
    "      2. For each page, send it to the API to improve its quality.\n",
    "      3. Return the combined, processed text.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing PDF {pdf_id}\")\n",
    "    pages = split_pat.split(pdf_text)\n",
    "    processed_pages = []\n",
    "    \n",
    "    for page_no, page in enumerate(pages):\n",
    "        if page.strip():\n",
    "            prompt = user_content + page.strip()\n",
    "            messages = [system, {\"role\": \"user\", \"content\": prompt}]\n",
    "            try:\n",
    "                processed_text = get_output(client=qwen_client, model=qwen_model, messages=messages)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error in PDF {pdf_id} page {page_no}: {e}\")\n",
    "                processed_text = page.strip()  # Fallback: use original text\n",
    "            processed_pages.append(processed_text)\n",
    "    return \"\\n\\n\".join(processed_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9db8b2-1e8f-4433-9da2-5d2c8342655a",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_file = \"checkpoint_results.json\"\n",
    "\n",
    "# Load checkpoint if it exists\n",
    "if os.path.exists(checkpoint_file):\n",
    "    with open(checkpoint_file, \"r\") as f:\n",
    "        checkpoint_data = json.load(f)\n",
    "else:\n",
    "    checkpoint_data = {}\n",
    "\n",
    "total_pdfs = len(jsonl_df)\n",
    "results = {int(k): v for k, v in checkpoint_data.items()}  # keys as int\n",
    "\n",
    "# Build a list of pdf indices that are not yet processed\n",
    "pending_ids = [i for i in range(total_pdfs) if i not in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8420ea-27d8-460f-b0ea-948c8bc3ecc3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process pending PDFs concurrently\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=128) as executor:  # Adjust as needed\n",
    "    future_to_pdf = {\n",
    "        executor.submit(process_pdf, idx, jsonl_df.iloc[idx][\"text\"]): idx\n",
    "        for idx in pending_ids\n",
    "    }\n",
    "    \n",
    "    for future in tqdm(as_completed(future_to_pdf), total=len(future_to_pdf), desc=\"Processing PDFs\"):\n",
    "        pdf_id = future_to_pdf[future]\n",
    "        try:\n",
    "            processed_text = future.result()\n",
    "            results[pdf_id] = processed_text\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing PDF {pdf_id}: {e}\")\n",
    "            results[pdf_id] = \"\"\n",
    "        # Write out checkpoint after each PDF completes\n",
    "        with open(checkpoint_file, \"w\") as f:\n",
    "            json.dump({str(k): v for k, v in results.items()}, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e7590-190d-4779-892a-486b18b19360",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "638060c6-2eb1-4e6f-855d-8f4df2ac6b38",
   "metadata": {},
   "source": [
    "# Convert to jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177e2561-9b93-489a-b523-efb944385d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../ppl_filtered_md.csv\"\n",
    "selected = pd.read_csv(path)\n",
    "selected_pdf = [os.path.join(\"..\", p) for p in selected[\"file_path\"].to_list()]\n",
    "print(f\"Num remaining pdf: {len(selected_pdf)}\")\n",
    "\n",
    "meta = glob.glob(\"../pdf_meta/*.csv\")\n",
    "df_meta = pd.concat([pd.read_csv(p) for p in meta], axis=0)\n",
    "\n",
    "col = [\n",
    "    \"title\",\n",
    "    \"pdf_link\",\n",
    "    \"source\",\n",
    "    \"license\",\n",
    "    \"filename\",\n",
    "]\n",
    "path = \"../pdf_meta_openbase/meta_openbase_2025-02-07_09_52_59.csv\"\n",
    "df_meta_openbase = pd.read_csv(path)\n",
    "df_meta_openbase = df_meta_openbase.loc[:, col].rename(columns={\"pdf_link\": \"url\"})\n",
    "\n",
    "df_meta = pd.concat([df_meta, df_meta_openbase], ignore_index=True)\n",
    "# df_meta.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb335ba0-aa03-46d8-9a8d-53316978c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_pat_1 = re.compile(r\"\\{\\d+\\}\\-{48}\\n\\n\")\n",
    "# remove_pat_2 = re.compile('<.*?>')\n",
    "json_list = []\n",
    "for i, path in enumerate(tqdm(selected_pdf)):\n",
    "    data = {}\n",
    "    pdf_file_name = path.split(\"/\")[-2] + \".pdf\"\n",
    "\n",
    "    text = open_md(path)\n",
    "    # text = remove_pat_1.sub(\"\", text)\n",
    "    # text = remove_pat_2.sub(\"\", text)\n",
    "\n",
    "    meta_data = df_meta.loc[df_meta[\"filename\"].eq(pdf_file_name)]\n",
    "\n",
    "    data[\"id\"] = str(i)\n",
    "    data[\"text\"] = text\n",
    "    data[\"source\"] = meta_data[\"source\"].values[0]\n",
    "\n",
    "    data[\"metadata\"] = {}\n",
    "    data[\"metadata\"][\"title\"] = meta_data[\"title\"].values[0]\n",
    "    data[\"metadata\"][\"url\"] = meta_data[\"url\"].values[0]\n",
    "    data[\"metadata\"][\"license\"] = meta_data[\"license\"].values[0]\n",
    "    json_list.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6209f2-1fcc-4be4-b446-d25201670cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(json_list).to_json(\n",
    "    f\"../pdf_data_{get_current_datetime()}_has_sep.jsonl\",\n",
    "    orient=\"records\",\n",
    "    lines=True,\n",
    "    force_ascii=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5aba29a-9c8b-4ab3-b517-e35a31e920b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76053e65-0e93-4185-80a2-7ac09d705bac",
   "metadata": {},
   "source": [
    "# tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df90ecc-bc0d-457e-b1e9-2268a106d997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_clone",
   "language": "python",
   "name": "ocr_clone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
