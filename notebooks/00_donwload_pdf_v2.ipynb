{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e432da7b-e0b0-436d-92da-3ac1a4a39399",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import subprocess\n",
    "import random\n",
    "import string\n",
    "import base64\n",
    "import glob\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util import Retry\n",
    "from pathlib import Path\n",
    "\n",
    "from io import BytesIO\n",
    "from pypdf import PdfReader\n",
    "from pypdf.errors import PdfReadError\n",
    "\n",
    "pd.options.display.max_rows = 500\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59f30d4",
   "metadata": {},
   "source": [
    "- Meta data is the most important thing. DON\"T MESS SUP META DATA !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cf42abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(os.listdir(\"../pdf_documents\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7f31741-e339-4ff0-8c91-2d92ee484628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_pdf(response, logger):\n",
    "    \"\"\"\n",
    "    Checks if the response content is a valid PDF.\n",
    "\n",
    "    Args:\n",
    "        response (requests.Response): The response object from requests.get()\n",
    "        logger (logging.Logger): Logger instance for logging messages.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if valid PDF, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if response.status_code != 200:\n",
    "            logger.error(f\"Invalid response status code: {response.status_code}\")\n",
    "            return False\n",
    "        \n",
    "        content_type = response.headers.get(\"Content-Type\", \"\").lower()\n",
    "        if \"pdf\" not in content_type:\n",
    "            logger.error(f\"Invalid content type: {content_type}\")\n",
    "            return False  # Content-Type should contain \"pdf\"\n",
    "\n",
    "        pdf_stream = BytesIO(response.content)\n",
    "        PdfReader(pdf_stream)  # Attempt to parse\n",
    "        return True\n",
    "    except PdfReadError:\n",
    "        logger.error(\"Failed to parse PDF. The file is not a valid PDF.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error while validating PDF: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def download_pdf(\n",
    "    session: requests.Session,\n",
    "    logger,\n",
    "    url: str,\n",
    "    save_path_str: str,\n",
    "    verify: bool = False,\n",
    "):\n",
    "    logger.info(f\"Starting download from: {url}\")\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, stream=True, verify=verify, timeout=20)\n",
    "        response.raise_for_status()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Request failed: {e}\")\n",
    "        return False\n",
    "\n",
    "    if not is_valid_pdf(response, logger):\n",
    "        logger.error(f\"Download aborted: {url} is not a valid PDF.\")\n",
    "        return False\n",
    "\n",
    "    save_path = Path(save_path_str)\n",
    "\n",
    "    try:\n",
    "        with save_path.open(\"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=1024):\n",
    "                if chunk:\n",
    "                    file.write(chunk)\n",
    "\n",
    "        logger.info(f\"File successfully downloaded to: {save_path}\")\n",
    "        return True\n",
    "    except OSError as e:\n",
    "        logger.error(f\"Failed to save file: {e}\")\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4944b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH_SAVE = \"../pdf_documents\"\n",
    "BASE_PATH_SAVE_META = \"../pdf_meta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62947e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wget_pdf(command: list) -> None:\n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(\"File downloaded successfully!\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def save_pdf_meta(meta: dict, meta_save_path: str):\n",
    "    pd.DataFrame([meta]).to_csv(meta_save_path, index=False)\n",
    "\n",
    "\n",
    "def generate_random_string(length=10):\n",
    "    return \"\".join(random.choices(string.ascii_letters + string.digits, k=length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aec3eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_to_code(input_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Converts a string into a deterministic, one-to-one code using Base64 encoding.\n",
    "\n",
    "    Args:\n",
    "        input_string (str): The input string to be converted.\n",
    "\n",
    "    Returns:\n",
    "        str: The encoded string as a deterministic code.\n",
    "    \"\"\"\n",
    "    # Encode the string into bytes\n",
    "    string_bytes = input_string.replace(\" \", \"\").encode(\"utf-8\")\n",
    "    # Convert the bytes to a Base64 encoded string\n",
    "    encoded_string = base64.urlsafe_b64encode(string_bytes).decode(\"utf-8\")\n",
    "    print(f\"lenght encode string: {len(encoded_string)}\")\n",
    "    if len(encoded_string) >= 255:\n",
    "        print(\"name too long, truncate name\")\n",
    "        encoded_string = encoded_string[-64:]\n",
    "    return encoded_string\n",
    "\n",
    "\n",
    "def code_to_string(encoded_string: str) -> str:\n",
    "    \"\"\"\n",
    "    Decodes the deterministic code back to the original string.\n",
    "\n",
    "    Args:\n",
    "        encoded_string (str): The encoded string to be decoded.\n",
    "\n",
    "    Returns:\n",
    "        str: The original string.\n",
    "    \"\"\"\n",
    "    # Decode the Base64 string back to bytes\n",
    "    string_bytes = base64.urlsafe_b64decode(encoded_string.encode(\"utf-8\"))\n",
    "    # Convert bytes back to a string\n",
    "    original_string = string_bytes.decode(\"utf-8\")\n",
    "    return original_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8640f704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenght encode string: 20\n",
      "Original String: Hello, World!?\n",
      "Encoded Code: SGVsbG8sV29ybGQhPw==\n",
      "Decoded String: Hello,World!?\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "original_string = \"Hello, World!?\"\n",
    "encoded = string_to_code(original_string)\n",
    "decoded = code_to_string(encoded)\n",
    "\n",
    "print(\"Original String:\", original_string)\n",
    "print(\"Encoded Code:\", encoded)\n",
    "print(\"Decoded String:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df4884b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_file(file_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Removes a file at the specified path.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the file to be removed.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the file exists\n",
    "        if os.path.exists(file_path):\n",
    "            # Remove the file\n",
    "            os.remove(file_path)\n",
    "            print(f\"File '{file_path}' has been removed successfully.\")\n",
    "        else:\n",
    "            print(f\"File '{file_path}' does not exist.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while trying to remove the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "86f427e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf_and_save_meta(\n",
    "    title: str,\n",
    "    url: str,\n",
    "    source: str,\n",
    "    license_: str,\n",
    "    filename: str,\n",
    "    command: list,\n",
    "    meta_save_path: str,\n",
    ") -> None:\n",
    "    pdf = {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"source\": source,\n",
    "        \"license\": license_,\n",
    "        \"filename\": filename,\n",
    "    }\n",
    "    print(\"meta view\")\n",
    "    pprint(pdf)\n",
    "\n",
    "    wget_pdf(command=command)\n",
    "    save_pdf_meta(meta=pdf, meta_save_path=meta_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22fdd0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_url = \"https://gdcatalog.go.th/dataset/gdpublish-dsb21-1\"\n",
    "# # Use a session to reuse the TCP connection for faster requests\n",
    "# session = requests.Session()\n",
    "\n",
    "# # Fetch and parse the main page\n",
    "# response = session.get(web_url)\n",
    "# response.raise_for_status()\n",
    "# soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# # Extract resource links with titles\n",
    "# base_url = \"https://gdcatalog.go.th\"\n",
    "# resources_section = soup.find(\"section\", id=\"dataset-resources\")\n",
    "# resource_links = [\n",
    "#     {\n",
    "#         \"title\": a[\"title\"].strip(),\n",
    "#         \"href\": base_url + a[\"href\"],\n",
    "#         \"source\": \"Goverment data catalog smart plus\",\n",
    "#         \"license\": \"Open Data Common\",\n",
    "#     }\n",
    "#     for a in resources_section.find_all(\"a\", class_=\"heading\")\n",
    "# ]\n",
    "\n",
    "# # Process each resource to extract the PDF URL\n",
    "# for i, resource in enumerate(tqdm(resource_links, desc=\"Processing resources\")):\n",
    "#     pdf_page = session.get(resource[\"href\"])\n",
    "#     pdf_page.raise_for_status()\n",
    "#     pdf_soup = BeautifulSoup(pdf_page.text, \"html.parser\")\n",
    "#     pdf_url = pdf_soup.find(\"a\", class_=\"resource-url-analytics\")[\"href\"]\n",
    "#     resource[\"url\"] = pdf_url\n",
    "#     resource[\"filename\"] = f\"pdf_มูลค่าและระดับการเติบโตของกิจกรรมทางเศรษฐกิจดิจิทัล_0{i}.pdf\"\n",
    "#     resource.pop(\"href\", None)\n",
    "\n",
    "# resource_links = list(filter(lambda x: x[\"url\"].endswith(\".pdf\"), resource_links))\n",
    "# pd.DataFrame(resource_links).to_csv(\n",
    "#     \"../pdf_documents_meta_tmp/pdf_มูลค่าและระดับการเติบโตของกิจกรรมทางเศรษฐกิจดิจิทัล.csv\",\n",
    "#     index=False,\n",
    "# )\n",
    "# for item in tqdm(resource_links, desc=\"donwloading pdf \"):\n",
    "#     download_pdf(\n",
    "#         url=item[\"url\"], save_path=os.path.join(BASE_PATH_SAVE, item[\"filename\"])\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f5c830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USE ONLY FOR GD DATACATALOG!!!\n",
    "BASE_GD_CATALOG_URL = \"https://gdcatalog.go.th\"\n",
    "GD_CATALOG_SOURCE = \"Government data catalog smart plus\"\n",
    "\n",
    "\n",
    "def fetch_resource_links(\n",
    "    request_session: requests.Session, web_url: str, license: str\n",
    ") -> list[dict]:\n",
    "    \"\"\"Fetch resource links from the given web URL.\"\"\"\n",
    "    response = request_session.get(web_url)\n",
    "    response.raise_for_status()\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    resources_section = soup.find(\"section\", id=\"dataset-resources\")\n",
    "    if not resources_section:\n",
    "        return []\n",
    "\n",
    "    return [\n",
    "        {\n",
    "            \"title\": a[\"title\"].strip(),\n",
    "            \"href\": BASE_GD_CATALOG_URL + a[\"href\"],\n",
    "            \"source\": GD_CATALOG_SOURCE,\n",
    "            \"license\": license,\n",
    "        }\n",
    "        for a in resources_section.find_all(\"a\", class_=\"heading\")\n",
    "    ]\n",
    "\n",
    "\n",
    "def extract_pdf_urls(\n",
    "    request_session: requests.Session, resource_links: list[dict], batch_code: str\n",
    "):\n",
    "    \"\"\"Extract PDF URLs from each resource page.\"\"\"\n",
    "    for i, resource in enumerate(tqdm(resource_links, desc=\"Processing resources\")):\n",
    "        pdf_page = request_session.get(resource[\"href\"])\n",
    "        pdf_page.raise_for_status()\n",
    "        pdf_soup = BeautifulSoup(pdf_page.text, \"html.parser\")\n",
    "        pdf_url_tag = pdf_soup.find(\"a\", class_=\"resource-url-analytics\")\n",
    "\n",
    "        if pdf_url_tag and \"href\" in pdf_url_tag.attrs:\n",
    "            resource[\"url\"] = pdf_url_tag[\"href\"]\n",
    "            resource[\"filename\"] = f\"pdf_{batch_code}_{i:05}.pdf\"\n",
    "\n",
    "        resource.pop(\"href\", None)\n",
    "\n",
    "    return list(filter(lambda x: x.get(\"url\", \"\").endswith(\".pdf\"), resource_links))\n",
    "\n",
    "\n",
    "def save_metadata(resource_links, output_path):\n",
    "    \"\"\"Save resource metadata to a CSV file.\"\"\"\n",
    "    df = pd.DataFrame(resource_links)\n",
    "    df.to_csv(output_path, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "def download_pdfs(resource_links, save_path):\n",
    "    \"\"\"Download PDFs from the extracted links.\"\"\"\n",
    "    for item in tqdm(resource_links, desc=\"Downloading PDFs\"):\n",
    "        download_pdf(url=item[\"url\"], save_path=save_path)\n",
    "\n",
    "\n",
    "def get_multiple_pdf_from_gd_catalog(\n",
    "    request_session: requests.Session, web_url: str, license: str, batch_code: str\n",
    ") -> list[dict]:\n",
    "    # Fetch and process resource links\n",
    "    resource_links = fetch_resource_links(request_session, web_url, license)\n",
    "    resource_links = extract_pdf_urls(request_session, resource_links, batch_code)\n",
    "    return resource_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c3e18a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pdf from open base\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3ea20651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-07_11_38_32\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_current_datetime():\n",
    "    # Get current date and time\n",
    "    now = datetime.now()\n",
    "    \n",
    "    # Format the date-time string\n",
    "    formatted_datetime = now.strftime(\"%Y-%m-%d_%H_%M_%S\")\n",
    "    \n",
    "    return formatted_datetime\n",
    "\n",
    "# Example usage\n",
    "print(get_current_datetime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ce291d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_filename_column(df: pd.DataFrame, batch_code: str) -> pd.DataFrame:\n",
    "    df[\"filename\"] = [f\"pdf_doc_{batch_code}_{i:05}.pdf\" for i in range(len(df))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28eb1cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger(\n",
    "    prefix: str,\n",
    "    log_file: str = None,\n",
    "    console_level: str = \"DEBUG\",\n",
    "    file_level: str = \"WARNING\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Sets up a logger with a console and file handler.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(\n",
    "        logging.DEBUG\n",
    "    )  # Set to the highest level; handlers will filter appropriately\n",
    "\n",
    "    # Create console handler\n",
    "    console_handler = logging.StreamHandler()\n",
    "    console_handler.setLevel(console_level)\n",
    "\n",
    "    log_file = f\"../log/{prefix}_{get_current_datetime()}.log\"\n",
    "\n",
    "    file_handler = logging.FileHandler(log_file, mode=\"a\", encoding=\"utf-8\")\n",
    "    file_handler.setLevel(file_level)\n",
    "\n",
    "    # Define log format\n",
    "    formatter = logging.Formatter(\n",
    "        \"{asctime},{levelname},{message}\",\n",
    "        style=\"{\",\n",
    "        datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    )\n",
    "\n",
    "    # Assign formatter to handlers\n",
    "    console_handler.setFormatter(formatter)\n",
    "    file_handler.setFormatter(formatter)\n",
    "\n",
    "    # Add handlers to logger\n",
    "    logger.addHandler(console_handler)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93bd3349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(102):  # Change the range as needed\n",
    "#     filename = f\"save_{i:03}\"  # Format the number as three digits with leading zeros\n",
    "#     print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8553942",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08e4ae32",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Single PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1995c8ca",
   "metadata": {},
   "source": [
    "- First, create a new meta file. Use the format like files in `pdf_meta` dir\n",
    "- Then, donwload each pdf using `download_pdf`\n",
    "- return pdf name and link that can not be downloaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c06a7bb",
   "metadata": {},
   "source": [
    "- meta data ตอนแรกน่าจะต้องทำมือโดยมี columns: title, url, source, license จากนั้นค่อยอ่านไฟล์มาสร้าง columns ใหม่คือ filename\n",
    "- ให้สร้างไฟล์ใน folder pdf_meta_pre\n",
    "- filename จะต้องมีเลข batch ด้วย ชื่อไฟล์จะได้ไม่ซ้ำกัน\n",
    "- จากนั้นให้ save dataframe meta แบบที่มี filename เข้าไปใน pdf_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a307d0",
   "metadata": {},
   "source": [
    "- folder pdf_meta ห้ามมีไฟล์ที่ไม่ใช้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4345e037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# files = os.listdir(\"../notuse_pdf_documents_meta_tmp\")\n",
    "# df  = pd.concat([pd.read_csv(os.path.join(\"../notuse_pdf_documents_meta_tmp\", f)) for f in files]).drop(columns=\"filename\")\n",
    "# filenames = [f\"pdf_doc_{i:05}.pdf\" for i in range(len(df))]\n",
    "# df[\"filename\"] = filenames\n",
    "# df.head(5)\n",
    "# df.to_csv(f\"../pdf_meta/meta_{get_current_datetime()}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bfed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = \"../pdf_meta/meta_2025-01-30_10_15_23.csv\" # อย่าลบบรรทัดนี้ ให้ comment out แทน\n",
    "path = \"../pdf_meta/meta_2025-01-31_09_18_50.csv\"\n",
    "meta = pd.read_csv(path).to_dict(orient=\"records\")\n",
    "meta[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e29b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_filename = [os.path.join(BASE_PATH_SAVE ,item[\"filename\"]) for item in meta]\n",
    "for f in delete_filename:\n",
    "    remove_file(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e63f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "\n",
    "session = requests.Session()\n",
    "retry = Retry(connect=3, backoff_factor=0.5)\n",
    "adapter = HTTPAdapter(max_retries=retry)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffa020e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_item = []\n",
    "\n",
    "for item in tqdm(meta, total=len(meta)):\n",
    "    pdf_save_path = os.path.join(BASE_PATH_SAVE, item[\"filename\"])\n",
    "    check = download_pdf(session=session, url=item[\"url\"], save_path=pdf_save_path)\n",
    "\n",
    "    sleep_time = random.randint(0, 20)\n",
    "    print(f\"sleep for {sleep_time} secs\")\n",
    "    time.sleep(sleep_time)\n",
    "\n",
    "    if not check:\n",
    "        fail_item.append((item[\"title\"], item[\"url\"], item[\"filename\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec05f404",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not fail_item:\n",
    "    print(\"every pdf donwloaded successfully\")\n",
    "else:\n",
    "    print(\"some pdf have a problem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e2762",
   "metadata": {},
   "outputs": [],
   "source": [
    "fail_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40be884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c705cf9a",
   "metadata": {},
   "source": [
    "# Multiple PDF for one site"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10277485",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## GD CATALOG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559b88f",
   "metadata": {},
   "source": [
    "- We can scape pdf from this web\n",
    "- use `get_multiple_pdf_from_gd_catalog` to get meta data for each website, then concat them\n",
    "- use `download_pdfs` to donwload all the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609146af",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c12a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_random_string(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf8f89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "# format: url, license, group\n",
    "# batch_code be anythoing just to distinguish file name\n",
    "# urls = [\n",
    "#     (\"https://gdcatalog.go.th/dataset/gdpublish-abc1-1\", \"Open Data Common\", \"A\"),\n",
    "#     (\"https://gdcatalog.go.th/dataset/gdpublish-xc6-4\", \"Open Data Common\", \"B\")\n",
    "# ]\n",
    "urls = [\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-itd65_42_01\",\n",
    "        \"Open Data Common\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-nsoloei\",\n",
    "        \"Open Data Common\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-https-www2-uttaradit-go-th-news_devpro\",\n",
    "        \"Open Data Common\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-nan01\",\n",
    "        \"Open Data Common\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-cop\",\n",
    "        \"Open Data Common\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-dataset-20-015\",\n",
    "        \"Open Data Common\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-dataset-15-013\",\n",
    "        \"Creative Commons Attributions\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-research-21-02\",\n",
    "        \"Open Data Common\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-ta-11-04\",\n",
    "        \"Open Data Common\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-ta-11-03\",\n",
    "        \"Open Data Common\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-ta-11-02\",\n",
    "        \"Open Data Common\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "    (\n",
    "        \"https://gdcatalog.go.th/dataset/gdpublish-ta-11-01\",\n",
    "        \"Open Data Common\",\n",
    "        generate_random_string(n),\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408643eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for url, license, batch_code in tqdm(urls):\n",
    "    resource_links = get_multiple_pdf_from_gd_catalog(\n",
    "        session, url, license, batch_code\n",
    "    )\n",
    "    tmp.extend(resource_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d7c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_order = [\"title\", \"url\", \"source\", \"license\", \"filename\"]\n",
    "df = pd.DataFrame(tmp).loc[:, col_order]\n",
    "df.to_csv(f\"../pdf_meta/meta_{get_current_datetime()}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b08113",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a282e2ba",
   "metadata": {},
   "source": [
    "## Special case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6d345f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### แจกเอกสารประกอบการสอนเพื่อการศึกษา (PDF) KongRuksiam Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef97ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = [\n",
    "    \"พัฒนาเว็บด้วย Python & Flask (Free).pdf\",\n",
    "    \"เรียนรู้การใช้งาน Git & GitHub สำหรับผู้เริ่มต้น.pdf\",\n",
    "    \"JSON เบื้องต้น (Update).pdf\",\n",
    "    \"Python & OpenCV Computer Vision & Image Processing (Free).pdf\",\n",
    "    \"รวมเล่มพัฒนาเว็บด้วย PHP สำหรับผู้เริ่มต้น.pdf\",\n",
    "    \"เรียนรู้การใช้งาน Visual Studio Code.pdf\",\n",
    "    \"ปูพื้นฐาน HTML CSS JavaScript (Free).pdf\",\n",
    "    \"รวมเล่มเขียนโปรแกรมภาษา C เบื้องต้น.pdf\",\n",
    "    \"รวมเล่มเขียนโปรแกรมภาษา Python (อัปเดตล่าสุด).pdf\",\n",
    "    \"เขียนโปรแกรมเชิงวัตถุด้วยภาษา Java (Update).pdf\",\n",
    "    \"Python OOP.pdf\",\n",
    "]\n",
    "\n",
    "url = [\n",
    "    \"https://github.com/kongruksiamza/ebook-for-education/blob/main/Flask%20Framework%20%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B9%89%E0%B8%99/%E0%B8%9E%E0%B8%B1%E0%B8%92%E0%B8%99%E0%B8%B2%E0%B9%80%E0%B8%A7%E0%B9%87%E0%B8%9A%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2%20Python%20%26%20Flask%20(Free).pdf\",\n",
    "    \"https://github.com/kongruksiamza/ebook-for-education/blob/main/Git%20%26%20GitHub%20%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B9%89%E0%B8%99/%E0%B9%80%E0%B8%A3%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B8%87%E0%B8%B2%E0%B8%99%20Git%20%26%20GitHub%20%E0%B8%AA%E0%B8%B3%E0%B8%AB%E0%B8%A3%E0%B8%B1%E0%B8%9A%E0%B8%9C%E0%B8%B9%E0%B9%89%E0%B9%80%E0%B8%A3%E0%B8%B4%E0%B9%88%E0%B8%A1%E0%B8%95%E0%B9%89%E0%B8%99.pdf\",\n",
    "    \"https://github.com/kongruksiamza/ebook-for-education/blob/main/JSON%20%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B9%89%E0%B8%99/JSON%20%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B9%89%E0%B8%99%20(Update).pdf\",\n",
    "    \"https://github.com/kongruksiamza/ebook-for-education/blob/main/OpenCV%20%26%20Python%20%E0%B8%AA%E0%B8%B3%E0%B8%AB%E0%B8%A3%E0%B8%B1%E0%B8%9A%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B8%9B%E0%B8%A3%E0%B8%B0%E0%B8%A1%E0%B8%A7%E0%B8%A5%E0%B8%9C%E0%B8%A5%E0%B8%A0%E0%B8%B2%E0%B8%9E%20(Image%20Processing)/Python%20%26%20OpenCV%20Computer%20Vision%20%26%20Image%20Processing%20(Free).pdf\",\n",
    "    \"https://github.com/kongruksiamza/ebook-for-education/blob/main/PHP%20%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B9%89%E0%B8%99/%E0%B8%A3%E0%B8%A7%E0%B8%A1%E0%B9%80%E0%B8%A5%E0%B9%88%E0%B8%A1%E0%B8%9E%E0%B8%B1%E0%B8%92%E0%B8%99%E0%B8%B2%E0%B9%80%E0%B8%A7%E0%B9%87%E0%B8%9A%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2%20PHP%20%E0%B8%AA%E0%B8%B3%E0%B8%AB%E0%B8%A3%E0%B8%B1%E0%B8%9A%E0%B8%9C%E0%B8%B9%E0%B9%89%E0%B9%80%E0%B8%A3%E0%B8%B4%E0%B9%88%E0%B8%A1%E0%B8%95%E0%B9%89%E0%B8%99.pdf\",\n",
    "    \"https://github.com/kongruksiamza/ebook-for-education/blob/main/Visual%20Studio%20Code%20%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B9%89%E0%B8%99/%E0%B9%80%E0%B8%A3%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B8%A3%E0%B8%B9%E0%B9%89%E0%B8%81%E0%B8%B2%E0%B8%A3%E0%B9%83%E0%B8%8A%E0%B9%89%E0%B8%87%E0%B8%B2%E0%B8%99%20Visual%20Studio%20Code.pdf\",\n",
    "    \"https://github.com/kongruksiamza/ebook-for-education/blob/main/%E0%B8%9E%E0%B8%B7%E0%B9%89%E0%B8%99%E0%B8%90%E0%B8%B2%E0%B8%99%20HTML%20%2CCSS%20%2C%20JavaScript/%E0%B8%9B%E0%B8%B9%E0%B8%9E%E0%B8%B7%E0%B9%89%E0%B8%99%E0%B8%90%E0%B8%B2%E0%B8%99%20HTML%20CSS%20JavaScript%20(Free).pdf\",\n",
    "    \"https://github.com/kongruksiamza/ebook-for-education/blob/main/%E0%B9%80%E0%B8%82%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B9%82%E0%B8%9B%E0%B8%A3%E0%B9%81%E0%B8%81%E0%B8%A3%E0%B8%A1%E0%B8%A0%E0%B8%B2%E0%B8%A9%E0%B8%B2%20C%20%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B9%89%E0%B8%99/%E0%B8%A3%E0%B8%A7%E0%B8%A1%E0%B9%80%E0%B8%A5%E0%B9%88%E0%B8%A1%E0%B9%80%E0%B8%82%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B9%82%E0%B8%9B%E0%B8%A3%E0%B9%81%E0%B8%81%E0%B8%A3%E0%B8%A1%E0%B8%A0%E0%B8%B2%E0%B8%A9%E0%B8%B2%20C%20%E0%B9%80%E0%B8%9A%E0%B8%B7%E0%B9%89%E0%B8%AD%E0%B8%87%E0%B8%95%E0%B9%89%E0%B8%99.pdf\",\n",
    "    \"https://github.com/kongruksiamza/ebook-for-education/blob/main/%E0%B9%80%E0%B8%82%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B9%82%E0%B8%9B%E0%B8%A3%E0%B9%81%E0%B8%81%E0%B8%A3%E0%B8%A1%E0%B8%A0%E0%B8%B2%E0%B8%A9%E0%B8%B2%20Python%20(%E0%B8%AD%E0%B8%B1%E0%B8%9B%E0%B9%80%E0%B8%94%E0%B8%95%E0%B8%A5%E0%B9%88%E0%B8%B2%E0%B8%AA%E0%B8%B8%E0%B8%94)/%E0%B8%A3%E0%B8%A7%E0%B8%A1%E0%B9%80%E0%B8%A5%E0%B9%88%E0%B8%A1%E0%B9%80%E0%B8%82%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B9%82%E0%B8%9B%E0%B8%A3%E0%B9%81%E0%B8%81%E0%B8%A3%E0%B8%A1%E0%B8%A0%E0%B8%B2%E0%B8%A9%E0%B8%B2%20Python%20(%E0%B8%AD%E0%B8%B1%E0%B8%9B%E0%B9%80%E0%B8%94%E0%B8%95%E0%B8%A5%E0%B9%88%E0%B8%B2%E0%B8%AA%E0%B8%B8%E0%B8%94).pdf\",\n",
    "    \"https://github.com/kongruksiamza/ebook-for-education/blob/main/%E0%B9%80%E0%B8%82%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B9%82%E0%B8%9B%E0%B8%A3%E0%B9%81%E0%B8%81%E0%B8%A3%E0%B8%A1%E0%B9%80%E0%B8%8A%E0%B8%B4%E0%B8%87%E0%B8%A7%E0%B8%B1%E0%B8%95%E0%B8%96%E0%B8%B8%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2%20Java/%E0%B9%80%E0%B8%82%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B9%82%E0%B8%9B%E0%B8%A3%E0%B9%81%E0%B8%81%E0%B8%A3%E0%B8%A1%E0%B9%80%E0%B8%8A%E0%B8%B4%E0%B8%87%E0%B8%A7%E0%B8%B1%E0%B8%95%E0%B8%96%E0%B8%B8%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2%E0%B8%A0%E0%B8%B2%E0%B8%A9%E0%B8%B2%20Java%20(Update).pdf\",\n",
    "    \"https://github.com/kongruksiamza/ebook-for-education/blob/main/%E0%B9%80%E0%B8%82%E0%B8%B5%E0%B8%A2%E0%B8%99%E0%B9%82%E0%B8%9B%E0%B8%A3%E0%B9%81%E0%B8%81%E0%B8%A3%E0%B8%A1%E0%B9%80%E0%B8%8A%E0%B8%B4%E0%B8%87%E0%B8%A7%E0%B8%B1%E0%B8%95%E0%B8%96%E0%B8%B8%E0%B8%94%E0%B9%89%E0%B8%A7%E0%B8%A2%20Python/Python%20OOP.pdf\",\n",
    "]\n",
    "source = [\"https://github.com/kongruksiamza/ebook-for-education\"] * len(filename)\n",
    "license = [\"CC BY-NC\"] * len(filename)\n",
    "title = [f.replace(\".pdf\", \"\") for f in filename]\n",
    "group = \"kongruksiam\"\n",
    "\n",
    "meta_save_path = os.path.join(\n",
    "    BASE_PATH_SAVE_META, f\"meta_{group}_{get_current_datetime()}.csv\"\n",
    ")\n",
    "df = pd.DataFrame(\n",
    "    {\n",
    "        \"title\": title,\n",
    "        \"url\": url,\n",
    "        \"source\": source,\n",
    "        \"license\": license,\n",
    "        \"filename\": filename,\n",
    "    }\n",
    ")\n",
    "df.to_csv(meta_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c144caa0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "735626c7",
   "metadata": {},
   "source": [
    "### Open BASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c358389",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL_OPENBASE = \"https://www.openbase.in.th\"\n",
    "session = requests.Session()\n",
    "RETRY_STRATEGY = Retry(\n",
    "    total=4,\n",
    "    backoff_factor=3,\n",
    "    status_forcelist=[500, 502, 503, 504, 404],\n",
    "    allowed_methods={\"POST\", \"GET\"},\n",
    ")\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=RETRY_STRATEGY))\n",
    "session.mount(\"http://\", HTTPAdapter(max_retries=RETRY_STRATEGY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e54574b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7266f6c5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Get Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00081f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.openbase.in.th/categories/search?page=1\"\n",
    "# response = session.get(url)\n",
    "# soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "# soup.find_all(\"ttt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f739f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "openbase_page_logger = setup_logger(prefix=\"openbase_page_logger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027df5c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROWS_INFO = []\n",
    "# ROWS_HREF = []\n",
    "# PAGES_NO = []\n",
    "\n",
    "# for i in trange(0, 278):\n",
    "#     url = f\"{BASE_URL_OPENBASE}/categories/search?page={i}\"\n",
    "\n",
    "#     try:\n",
    "#         response = session.get(url)\n",
    "#         response.raise_for_status()\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         openbase_page_logger.error(f\"Error fetching {url}: {e}\")\n",
    "#         continue  # Skip to the next iteration\n",
    "\n",
    "#     soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "#     # Find all tables and check if they exist\n",
    "#     tables = soup.find_all(\"table\")\n",
    "#     if not tables:\n",
    "#         openbase_page_logger.warning(f\"No tables found on page {i}\")\n",
    "#         continue\n",
    "\n",
    "#     last_table = tables[-1]\n",
    "#     tbody = last_table.find(\"tbody\")\n",
    "\n",
    "#     if not tbody:\n",
    "#         openbase_page_logger.warning(f\"No tbody found in table on page {i}\")\n",
    "#         continue\n",
    "\n",
    "#     rows = tbody.find_all(\"tr\")\n",
    "#     if not rows:\n",
    "#         openbase_page_logger.warning(f\"No rows found in tbody on page {i}\")\n",
    "#         continue\n",
    "\n",
    "#     # Extract table data and links\n",
    "#     rows_info = [[cell.text.strip() for cell in row.find_all(\"td\")] for row in rows]\n",
    "#     rows_href = [\n",
    "#         BASE_URL_OPENBASE + row.find(\"a\").get(\"href\", \"\") if row.find(\"a\") else None\n",
    "#         for row in rows\n",
    "#     ]\n",
    "\n",
    "#     ROWS_INFO.extend(rows_info)\n",
    "#     ROWS_HREF.extend(filter(None, rows_href))  # Removes None values\n",
    "#     PAGES_NO.extend([i + 1] * len(rows_href))\n",
    "\n",
    "# print(f\"Scraped {len(ROWS_INFO)} rows of info and {len(ROWS_HREF)} links successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc3c3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def openbase_scrape_pages(session, logger, max_pages=278):\n",
    "    \"\"\"Scrape data from paginated category search results.\n",
    "\n",
    "    Args:\n",
    "        session (requests.Session): A configured requests session.\n",
    "        logger (logging.Logger): Logger for error tracking.\n",
    "        max_pages (int): Number of pages to scrape (default: 278).\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing extracted data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Storage for extracted data\n",
    "    rows_info_list = []\n",
    "    rows_href_list = []\n",
    "    pages_no_list = []\n",
    "\n",
    "    for page_num in trange(0, max_pages, desc=\"Scraping Pages\"):\n",
    "        url = f\"{BASE_URL_OPENBASE}/categories/search?page={page_num}\"\n",
    "\n",
    "        try:\n",
    "            response = session.get(url)\n",
    "            response.raise_for_status()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logger.error(f\"Error fetching {url}: {e}\")\n",
    "            continue  # Skip to next page\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Extract last table on the page\n",
    "        last_table = soup.find_all(\"table\")[-1] if soup.find_all(\"table\") else None\n",
    "        if not last_table:\n",
    "            logger.warning(f\"No tables found on page {page_num}\")\n",
    "            continue\n",
    "\n",
    "        tbody = last_table.find(\"tbody\")\n",
    "        if not tbody:\n",
    "            logger.warning(f\"No tbody found in table on page {page_num}\")\n",
    "            continue\n",
    "\n",
    "        rows = tbody.find_all(\"tr\")\n",
    "        if not rows:\n",
    "            logger.warning(f\"No rows found in tbody on page {page_num}\")\n",
    "            continue\n",
    "\n",
    "        # Extract table data and links\n",
    "        rows_info = [[cell.text.strip() for cell in row.find_all(\"td\")] for row in rows]\n",
    "        rows_href = [\n",
    "            (\n",
    "                f\"{BASE_URL_OPENBASE}{row.find('a').get('href', '')}\"\n",
    "                if row.find(\"a\")\n",
    "                else None\n",
    "            )\n",
    "            for row in rows\n",
    "        ]\n",
    "\n",
    "        # Store data, removing None values from hrefs\n",
    "        rows_info_list.extend(rows_info)\n",
    "        rows_href_list.extend(filter(None, rows_href))\n",
    "        pages_no_list.extend([page_num + 1] * len(rows_href))\n",
    "\n",
    "    logger.info(\n",
    "        f\"Scraped {len(rows_info_list)} rows of info and {len(rows_href_list)} links successfully.\"\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"rows_info\": rows_info_list,\n",
    "        \"rows_href\": rows_href_list,\n",
    "        \"pages_no\": pages_no_list,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58dd1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_INFO = openbase_scrape_pages(session, openbase_page_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6cf2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ROW_INFO.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02df8d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"title\", \"category\", \"last_update\", \"create_date\"]\n",
    "df = pd.DataFrame(ROW_INFO[\"rows_info\"], columns=cols)\n",
    "df[\"link\"] = ROW_INFO[\"rows_href\"]\n",
    "df[\"page_no\"] = ROW_INFO[\"pages_no\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b05028a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2c5263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[\"category\"].value_counts()\n",
    "# df.loc[df[\"category\"].eq(\"\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0864fcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cd3177",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_save_path = os.path.join(\n",
    "    \"../pdf_meta_openbase\", \n",
    "    f\"meta_openbase_link_to_pdfs_{get_current_datetime()}.csv\"\n",
    ")\n",
    "df.to_csv(meta_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37963d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f601e94",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Get PDF Link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fd17d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../pdf_meta_openbase/meta_openbase_link_to_pdfs_2025-02-06_18_45_09.csv\"\n",
    "df = pd.read_csv(path); print(df.shape)\n",
    "links = df[\"link\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffce247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7145a14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a5a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define session and regex pattern for PDFs\n",
    "# PDF_LINK_PAT = re.compile(r'href=\"([^\"]+\\.pdf)\"')  # Extracts href content directly\n",
    "# MAPPING = {}\n",
    "\n",
    "# for url in tqdm(links):\n",
    "#     for attempt in range(2):  # Try twice before failing\n",
    "#         try:\n",
    "#             response = session.get(url, timeout=30)\n",
    "#             response.raise_for_status()\n",
    "#             break  # If successful, exit retry loop\n",
    "#         except requests.exceptions.RequestException as e:\n",
    "#             if attempt == 0:\n",
    "#                 time.sleep(5)  # Retry after delay\n",
    "#             else:\n",
    "#                 print(f\"Failed to fetch {url}: {e}\")\n",
    "#                 MAPPING[url] = []  # Store an empty list for failed URLs\n",
    "#                 continue\n",
    "\n",
    "#     pdf_links = PDF_LINK_PAT.findall(response.text)\n",
    "#     if pdf_links:\n",
    "#         pdf_links = [\n",
    "#             BASE_URL_OPENBASE + match for match in pdf_links if match.endswith(\".pdf\")\n",
    "#         ]  # can still be an empty list if there is no pdf\n",
    "#         MAPPING[url] = pdf_links\n",
    "#     else:\n",
    "#         MAPPING[url] = []\n",
    "\n",
    "\n",
    "# print(f\"Scraped {len(MAPPING)} pages successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6940041",
   "metadata": {},
   "outputs": [],
   "source": [
    "openbase_pdf_logger = setup_logger(\"openbase_get_pdf_link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588babc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Constants\n",
    "# PDF_LINK_PAT = re.compile(r'href=\"([^\"]+\\.pdf)\"')  # Extracts href content directly\n",
    "\n",
    "# def fetch_pdfs(url) -> tuple[str, list]:\n",
    "#     \"\"\"Fetch a URL and extract PDF links.\"\"\"\n",
    "#     try:\n",
    "#         response = session.get(url, timeout=10)  # Lower timeout for responsiveness\n",
    "#         response.raise_for_status()\n",
    "#         pdf_links = [\n",
    "#             BASE_URL_OPENBASE + match for match in PDF_LINK_PAT.findall(response.text)\n",
    "#         ]\n",
    "#         return url, pdf_links\n",
    "#     except requests.exceptions.RequestException as e:\n",
    "#         openbase_pdf_logger.error(f\"Failed to fetch {url}: {e}\")\n",
    "#         return url, []\n",
    "\n",
    "\n",
    "# def scrape_all(links, max_workers=10) -> dict[str, list]:\n",
    "#     \"\"\"Scrape all links concurrently.\"\"\"\n",
    "#     MAPPING = {}\n",
    "\n",
    "#     with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "#         future_to_url = {executor.submit(fetch_pdfs, url): url for url in links}\n",
    "\n",
    "#         for future in tqdm(\n",
    "#             as_completed(future_to_url), total=len(links), desc=\"Scraping PDFs\"\n",
    "#         ):\n",
    "#             url, pdf_links = future.result()\n",
    "#             MAPPING[url] = pdf_links\n",
    "\n",
    "#     openbase_pdf_logger.info(f\"Scraped {len(MAPPING)} pages successfully.\")\n",
    "#     return MAPPING\n",
    "\n",
    "\n",
    "# pdf_mapping = scrape_all(links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d78b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "PDF_LINK_PAT = re.compile(r'href=\"([^\"]+\\.pdf)\"')  # Extracts href content directly\n",
    "\n",
    "\n",
    "def fetch_pdfs(url: str, session: requests.Session, logger) -> tuple[str, list]:\n",
    "    \"\"\"Fetch a URL and extract PDF links.\n",
    "\n",
    "    Args:\n",
    "        url (str): The URL to scrape.\n",
    "        session (requests.Session): A session object for efficient requests.\n",
    "        logger (logging.Logger): Logger for logging errors and info.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, list]: The URL and a list of extracted PDF links.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = session.get(url, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        # pdf_links = [match for match in PDF_LINK_PAT.findall(response.text)]\n",
    "        pdf_links = PDF_LINK_PAT.findall(response.text)\n",
    "\n",
    "        return url, pdf_links\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logger.error(f\"Failed to fetch {url}: {e}\")\n",
    "        return url, []\n",
    "\n",
    "\n",
    "def openbase_scrape_pdf(\n",
    "    links: list, session: requests.Session, logger, max_workers=10\n",
    ") -> dict[str, list]:\n",
    "    \"\"\"Scrape multiple URLs concurrently to extract PDF links.\n",
    "\n",
    "    Args:\n",
    "        links (list): List of URLs to scrape.\n",
    "        session (requests.Session): A requests session for efficient network calls.\n",
    "        base_url (str): The base URL to prepend to relative links.\n",
    "        logger (logging.Logger): Logger for logging progress.\n",
    "        max_workers (int, optional): Number of concurrent threads. Defaults to 10.\n",
    "\n",
    "    Returns:\n",
    "        dict[str, list]: A dictionary mapping each URL to its extracted PDF links.\n",
    "    \"\"\"\n",
    "    pdf_mapping = {}\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        future_to_url = {\n",
    "            executor.submit(fetch_pdfs, url, session, logger): url for url in links\n",
    "        }\n",
    "\n",
    "        for future in tqdm(\n",
    "            as_completed(future_to_url),\n",
    "            total=len(links),\n",
    "            desc=\"Scraping PDFs\",\n",
    "            unit=\"page\",\n",
    "        ):\n",
    "            url, pdf_links = future.result()\n",
    "            pdf_mapping[url] = pdf_links\n",
    "\n",
    "    logger.info(f\"Scraped {len(pdf_mapping)} pages successfully.\")\n",
    "    return pdf_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c5f7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_mapping = openbase_scrape_pdf(links, session, openbase_pdf_logger)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a908419c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pdf_link\"] = df[\"link\"].map(pdf_mapping)\n",
    "df[\"pdf_link\"] = df[\"pdf_link\"].apply(lambda x: x if x else pd.NA)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6249b96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(f\"./openbase_pdf_link_tmp_{get_current_datetime()}.parquet\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267439e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=\"pdf_link\").explode(\"pdf_link\")\n",
    "df[\"source\"] = \"openbase.in.th\"\n",
    "df[\"license\"] = \"CC-BY-NC-SA\"\n",
    "df.loc[\n",
    "    ~(\n",
    "        df[\"pdf_link\"].str.startswith(\"https://\")\n",
    "        | df[\"pdf_link\"].str.startswith(\"http://\")\n",
    "    ),\n",
    "    \"pdf_link\",\n",
    "] = (\n",
    "    BASE_URL_OPENBASE\n",
    "    + df.loc[\n",
    "        ~(\n",
    "            df[\"pdf_link\"].str.startswith(\"https://\")\n",
    "            | df[\"pdf_link\"].str.startswith(\"http://\")\n",
    "        ),\n",
    "        \"pdf_link\",\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def create_filename_column(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"filename\"] = [f\"pdf_doc_openbase_{i:05}.pdf\" for i in range(len(df))]\n",
    "    return df\n",
    "\n",
    "\n",
    "df = create_filename_column(df)\n",
    "df = df.loc[~df[\"pdf_link\"].str.contains(\"thailife\", regex=False)]\n",
    "df = df.drop_duplicates(subset=\"pdf_link\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4671a8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n",
    "df[\"category\"] = df[\"category\"].fillna(\"NOCATEGORY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56574a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_save_path = os.path.join(\n",
    "    \"../pdf_meta_openbase\", \n",
    "    f\"meta_openbase_{get_current_datetime()}.csv\"\n",
    ")\n",
    "df.to_csv(meta_save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c14b6a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b32f911d",
   "metadata": {},
   "source": [
    "#### Download PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884b3476",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../pdf_meta_openbase/meta_openbase_2025-02-07_09_52_59.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642788a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = df.to_dict(orient=\"records\")\n",
    "meta[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413c02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open_base_pdf_download_logger = setup_logger(\"openbase_pdf_download_logger\")\n",
    "# fail_item = []\n",
    "# BASE_PATH_SAVE_OPENBASE = \"../pdf_documents_openbase\"\n",
    "\n",
    "# for item in tqdm(meta, total=len(meta)):\n",
    "#     pdf_save_path = os.path.join(BASE_PATH_SAVE_OPENBASE, item[\"filename\"])\n",
    "#     check = download_pdf(\n",
    "#         session=session,\n",
    "#         logger=open_base_pdf_download_logger,\n",
    "#         url=item[\"pdf_link\"],\n",
    "#         save_path_str=pdf_save_path,\n",
    "#         verify=True\n",
    "#     )\n",
    "\n",
    "#     sleep_time = random.randint(0, 3)\n",
    "#     open_base_pdf_download_logger.info(f\"sleep for {sleep_time} secs\")\n",
    "#     time.sleep(sleep_time)\n",
    "\n",
    "#     if not check:\n",
    "#         fail_item.append((item[\"title\"], item[\"pdf_link\"], item[\"filename\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddce78c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure the directory exists\n",
    "open_base_pdf_download_logger = setup_logger(\"openbase_pdf_download_logger\")\n",
    "BASE_PATH_SAVE_OPENBASE = Path(\"../pdf_documents_openbase\")\n",
    "fail_items = []\n",
    "\n",
    "\n",
    "def download_pdf_task(item):\n",
    "    \"\"\"Task function to download a PDF.\"\"\"\n",
    "    pdf_save_path = BASE_PATH_SAVE_OPENBASE / item[\"filename\"]\n",
    "\n",
    "    check = download_pdf(\n",
    "        session=session,\n",
    "        logger=open_base_pdf_download_logger,\n",
    "        url=item[\"pdf_link\"],\n",
    "        save_path_str=str(pdf_save_path),\n",
    "        verify=True,\n",
    "    )\n",
    "\n",
    "    if not check:\n",
    "        fail_items.append((item[\"title\"], item[\"pdf_link\"], item[\"filename\"]))\n",
    "        open_base_pdf_download_logger.error(\n",
    "            f\"Failed to download: {item['title']} ({item['pdf_link']})\"\n",
    "        )\n",
    "\n",
    "    return item[\"filename\"]  # Return filename for progress tracking\n",
    "\n",
    "\n",
    "# Define the number of concurrent threads\n",
    "MAX_WORKERS = 16  # Adjust based on your system/network\n",
    "\n",
    "# Run downloads in parallel\n",
    "with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "    futures = {executor.submit(download_pdf_task, item): item for item in meta}\n",
    "\n",
    "    for future in tqdm(as_completed(futures), total=len(meta), desc=\"Downloading PDFs\"):\n",
    "        try:\n",
    "            filename = future.result()\n",
    "            open_base_pdf_download_logger.info(f\"Completed: {filename}\")\n",
    "        except Exception as e:\n",
    "            open_base_pdf_download_logger.error(f\"Error during download: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f7ef2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d0931ef48e404887c8729cd8442ae9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4873 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n",
      "Ignoring wrong pointing object 0 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "incorrect startxref pointer(2)\n",
      "parsing for Object Streams\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 523 0 (offset 0)\n",
      "Ignoring wrong pointing object 0 0 (offset 0)\n",
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n",
      "parsing for Object Streams\n",
      "incorrect startxref pointer(2)\n",
      "parsing for Object Streams\n",
      "incorrect startxref pointer(2)\n",
      "parsing for Object Streams\n",
      "Ignoring wrong pointing object 21 0 (offset 0)\n",
      "incorrect startxref pointer(1)\n",
      "parsing for Object Streams\n",
      "Ignoring wrong pointing object 0 0 (offset 0)\n",
      "Ignoring wrong pointing object 25 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 29 0 (offset 0)\n",
      "Ignoring wrong pointing object 51 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 82 0 (offset 0)\n",
      "Ignoring wrong pointing object 349 0 (offset 0)\n",
      "Ignoring wrong pointing object 366 0 (offset 0)\n",
      "Ignoring wrong pointing object 368 0 (offset 0)\n",
      "Ignoring wrong pointing object 370 0 (offset 0)\n",
      "Ignoring wrong pointing object 372 0 (offset 0)\n",
      "Ignoring wrong pointing object 390 0 (offset 0)\n",
      "Ignoring wrong pointing object 392 0 (offset 0)\n",
      "Ignoring wrong pointing object 597 0 (offset 0)\n",
      "Ignoring wrong pointing object 614 0 (offset 0)\n",
      "Ignoring wrong pointing object 616 0 (offset 0)\n",
      "Ignoring wrong pointing object 618 0 (offset 0)\n",
      "Ignoring wrong pointing object 620 0 (offset 0)\n",
      "Ignoring wrong pointing object 638 0 (offset 0)\n",
      "Ignoring wrong pointing object 640 0 (offset 0)\n",
      "Ignoring wrong pointing object 946 0 (offset 0)\n",
      "Ignoring wrong pointing object 948 0 (offset 0)\n",
      "Ignoring wrong pointing object 950 0 (offset 0)\n",
      "Ignoring wrong pointing object 952 0 (offset 0)\n",
      "Ignoring wrong pointing object 954 0 (offset 0)\n",
      "Ignoring wrong pointing object 970 0 (offset 0)\n",
      "Ignoring wrong pointing object 972 0 (offset 0)\n",
      "Ignoring wrong pointing object 1263 0 (offset 0)\n",
      "Ignoring wrong pointing object 1280 0 (offset 0)\n",
      "Ignoring wrong pointing object 1282 0 (offset 0)\n",
      "Ignoring wrong pointing object 1284 0 (offset 0)\n",
      "Ignoring wrong pointing object 1286 0 (offset 0)\n",
      "Ignoring wrong pointing object 1288 0 (offset 0)\n",
      "Ignoring wrong pointing object 1306 0 (offset 0)\n",
      "Ignoring wrong pointing object 1308 0 (offset 0)\n",
      "Ignoring wrong pointing object 1409 0 (offset 0)\n",
      "Ignoring wrong pointing object 1411 0 (offset 0)\n",
      "Ignoring wrong pointing object 1560 0 (offset 0)\n",
      "Ignoring wrong pointing object 1562 0 (offset 0)\n",
      "Ignoring wrong pointing object 1564 0 (offset 0)\n",
      "Ignoring wrong pointing object 1566 0 (offset 0)\n",
      "Ignoring wrong pointing object 1673 0 (offset 0)\n",
      "Ignoring wrong pointing object 1675 0 (offset 0)\n"
     ]
    }
   ],
   "source": [
    "pdf_files = os.listdir(\"../pdf_documents_openbase\")\n",
    "bool_list = []\n",
    "for pdf_f in tqdm(pdf_files):\n",
    "    path = os.path.join(\"../pdf_documents_openbase\", pdf_f)\n",
    "    try:\n",
    "        PdfReader(path)\n",
    "        bool_list.append(True)\n",
    "    except PdfReadError:\n",
    "        bool_list.append(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "435035a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    4873\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(bool_list).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abe607b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b5932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41e25d2f",
   "metadata": {},
   "source": [
    "# Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9b9d55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5610012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb47ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
